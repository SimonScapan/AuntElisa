{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Kopie von nmt_with_attention.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZEnS5wOkz89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, TimeDistributed\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Bidirectional, Concatenate, Lambda\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "snapshot_folder = '/content/drive/My Drive/Data Exploration Project'\n",
        "\n",
        "\n",
        "test = False\n",
        "print('test: ', test)\n",
        "if test:\n",
        "    GRU_units = 10\n",
        "    batch_size = 4\n",
        "    emb_dim = 10\n",
        "else:\n",
        "    GRU_units = 256\n",
        "    batch_size = 32\n",
        "    emb_dim = 50\n",
        "\n",
        "init_lr = 0.0005"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykKMzwGTkz9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def progressBar(value, endvalue, bar_length=20, job='Job'):\n",
        "\n",
        "    percent = float(value) / endvalue\n",
        "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
        "    spaces = ' ' * (bar_length - len(arrow))\n",
        "\n",
        "    sys.stdout.write(\"\\r{0} Completion: [{1}] {2}%\".format(job,arrow + spaces, int(round(percent * 100))))\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VufTwi7S--pU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the preprocessed data\n",
        "with open('/content/drive/My Drive/Data Exploration Project/preprocessed_data.pkl', 'rb') as f:\n",
        "        preprocessed_data = pickle.load(f)\n",
        "\n",
        "wordtoix = preprocessed_data['word2ix']\n",
        "ixtoword = preprocessed_data['ixtoword']\n",
        "pairs_final_train = preprocessed_data['pairs_final_train']\n",
        "short_vocab = preprocessed_data['short_vocab']\n",
        "\n",
        "max_len_q = preprocessed_data['max_len_q']\n",
        "max_len_a = preprocessed_data['max_len_q']\n",
        "\n",
        "end_token = ' <EOS>'\n",
        "start_token = '<BOS> '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGdbxg0Xkz9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Since index 0 is used as padding, we have to increase the vocab size\n",
        "vocab_len = len(short_vocab) + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfWlnSCLkz9s",
        "colab_type": "text"
      },
      "source": [
        "#### making the model (Ã¼bernehmen!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCyugRNGkz9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #Making the embedding mtrix and decide whether to use pretrained word embeddings\n",
        "def make_embedding_layer(embedding_dim=100, glove=True):\n",
        "    if glove == False:\n",
        "        print('Just a zero matrix loaded')\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # just a zero matrix \n",
        "    else:\n",
        "        print('Loading glove...')\n",
        "        embeddings_index = {} \n",
        "        f = open('/content/drive/My Drive/LSTM_seq2seq_chatbot_with_keras/glove.6B.50d.txt', encoding=\"utf-8\")\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "        print(\"GloVe \",embedding_dim, ' loded!')\n",
        "        #Get 200 dimensions dense vector for each of the vocab_rocc\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) #To import as weights for Keras Embedding layer\n",
        "        for word, i in wordtoix.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                #Words not found in the embedding index will be all zeros\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=True, trainable=False) #We have a limited vocab so we \n",
        "                                                                                           #Do not train the embedding layer\n",
        "                                                                                           #We use 0 as padding so mask_zero as True\n",
        "    embedding_layer.build((None,))\n",
        "    embedding_layer.set_weights([embedding_matrix])\n",
        "    \n",
        "    return embedding_layer\n",
        "\n",
        "embeddings = make_embedding_layer(embedding_dim=emb_dim, glove=not test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "#Define the encoder of the network\n",
        "class Encoder(tf.keras.Model):\n",
        "    #Define the parameters of the class\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.embeddings = embeddings\n",
        "        \n",
        "        self.Bidirectional1 = Bidirectional(GRU(enc_units, return_sequences=True,\n",
        "                               return_state=False, recurrent_initializer='glorot_uniform', name='gru_1'), name='bidirectional_encoder1')\n",
        "        self.Bidirectional2 = Bidirectional(GRU(enc_units, return_sequences=True, \n",
        "                               return_state=True, recurrent_initializer='glorot_uniform', name='gru_2'), name='bidirectional_encoder2')\n",
        "                                                                                                \n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.Inp = Input(shape=(max_len_q,)) # size of questions\n",
        "\n",
        "    #Create bidirectional LSTM for easier access of information for previous and following layers \n",
        "    def bidirectional(self, bidir, layer, inp, hidden):\n",
        "        return bidir(layer(inp, initial_state = hidden))\n",
        "    \n",
        "    #Create Embedding with dropout to reduce overfitting and increase model performance\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        output, state_f,state_b = self.Bidirectional2(x)\n",
        "\n",
        "        return output, state_f, state_b\n",
        "\n",
        "    #Create an empty (or zero) tensor \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60gSVh05Jl6l",
        "colab": {}
      },
      "source": [
        "#Create encoder\n",
        "encoder = Encoder(vocab_len, emb_dim, GRU_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umohpBN2OM94",
        "colab": {}
      },
      "source": [
        "#Create bahdanau attention in the network to find the correlation between massage and response\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    #Define the parameters of the attention class\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        self.units = units\n",
        "        \n",
        "    def call(self, query, values):\n",
        "        #Broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "        \n",
        "        #Create attention weights\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        #Apply attention weights\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "#Define the decoder of the network\n",
        "class Decoder(tf.keras.Model):\n",
        "    #Define the parameters of the decoder class\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_size\n",
        "        self.embeddings = embeddings\n",
        "        self.units = 2 * dec_units #As we are using an bidirectional encoder\n",
        "        self.fc = Dense(vocab_len, activation='softmax', name='dense_layer')\n",
        "        #Use attention to improve creating responses\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "        self.decoder_gru_l1 = GRU(self.units, return_sequences=True, \n",
        "                                  return_state= False, recurrent_initializer='glorot_uniform' ,name='decoder_gru1')\n",
        "        self.decoder_gru_l2 = GRU(self.units, return_sequences=False, \n",
        "                                  return_state= True, recurrent_initializer='glorot_uniform' ,name='decoder_gru2') \n",
        "        self.dropout = Dropout(0.4)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        #Get the attention weights\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        x = self.embeddings(x)\n",
        "\n",
        "        #Concat input and context vector together\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) \n",
        "\n",
        "        #Passing the concatenated vector to the GRU\n",
        "        x = self.decoder_gru_l1(x)\n",
        "        x = self.dropout(x)\n",
        "        output, state = self.decoder_gru_l2(x)\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(vocab_len, emb_dim, GRU_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCw52uJWkz-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "#Similar to the training loop, but without teacher forcing here. Input to the decoder at each time step is its previous predictions with the hidden state and encoder output\n",
        "def evaluate(sentence):\n",
        "    #Create a empty (or zero) matrix\n",
        "    attention_plot = np.zeros((max_len_a, max_len_q))\n",
        "\n",
        "    #Clean the input message\n",
        "    sentence = clean_text(sentence)\n",
        "\n",
        "    #Translate the message to its word indices with start and end token and fill with padding tokens\n",
        "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
        "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
        "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    #Create an empty hidden GRU layer (uses sigmoid activation)\n",
        "    hidden = [tf.zeros((1, GRU_units))]\n",
        "\n",
        "    #Apply the encoder on the input message \n",
        "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
        "\n",
        "    #Conncatenate enc_hidden_f and enc_hidden_b\n",
        "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "\n",
        "    #Add a new dimension for the index of the start token\n",
        "    dec_input = tf.expand_dims([wordtoix[start_token]], 1)\n",
        "\n",
        "\n",
        "    for t in range(max_len_a):\n",
        "        #Use the decoder to create predictions\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        #Reshape the attention weights\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "\n",
        "        #Get all attention weights for the word in the current loop\n",
        "        attention_plot[t] = K.get_value(attention_weights)\n",
        "        \n",
        "        #Get most probable next word based on the word in the current loop and previous predicted words\n",
        "        predicted_id =  K.get_value(tf.argmax(predictions[0]))       \n",
        "\n",
        "        #Stop prediction, if we reached the end token\n",
        "        if ixtoword[predicted_id] == end_token:\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        #Add the predicted word to the response\n",
        "        result += ixtoword[predicted_id] + ' '\n",
        "\n",
        "        #Put the predicted word back to the model as we are using RNN\n",
        "        dec_input = tf.expand_dims([predicted_id], 1)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5hQWlbN3jGF",
        "colab": {}
      },
      "source": [
        "#Function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(5,5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sl9zUHzg3jGI",
        "colab": {}
      },
      "source": [
        "#Retrieve the response based on the message\n",
        "def answer(sentence, training=False):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    if training:\n",
        "        return result\n",
        "    \n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted answer: {}'.format(result))\n",
        "    attention_plot = attention_plot[1:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' ')[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "#Applying the adam optimizer to update the network weights\n",
        "optimizer = tf.keras.optimizers.Adam(init_lr)\n",
        "\n",
        "#Define the loss function\n",
        "def loss_function(real, pred):\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = K.sparse_categorical_crossentropy(real, pred, from_logits= False)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "#Initalize a checkpoint to save the model later\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "#Define the tensorflow trining function\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        #Call the Encoder class\n",
        "        enc_output, enc_hidden_f, enc_hidden_b = encoder(inp, enc_hidden)\n",
        "        \n",
        "        #Conncatenate enc_hidden_f and enc_hidden_b\n",
        "        dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "\n",
        "        #Add a new dimension for the index of the start token\n",
        "        dec_input = tf.expand_dims([wordtoix[start_token]] * batch_size, 1)\n",
        "\n",
        "        #Teacher forcing (feeding the target as the next input)\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            \n",
        "            #Passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            #Each time just use one timestep output\n",
        "            loss += loss_function(targ[:, t], predictions) \n",
        "            #Expected output at this time becomes input for next timestep\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1) \n",
        "            \n",
        "    #Calculate the loss of this batch\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    #Calculate the gradients in respect to the variables\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #Apply the gradient to the optimizer to update the weights\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRU6XGlUkz-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define and calculate hyperparameters for training\n",
        "history={'loss':[]}\n",
        "smallest_loss = np.inf\n",
        "best_ep = 1\n",
        "EPOCHS = 5 # but 150 is enough\n",
        "enc_hidden = encoder.initialize_hidden_state()\n",
        "steps_per_epoch = len(pairs_final_train)//batch_size # used for caculating number of batches\n",
        "current_ep = 1\n",
        "batch_per_epoche = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gamR6-S3kz-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a test which is printed after each train epoche\n",
        "def test_bot():\n",
        "    print('#'*20)\n",
        "    q = 'Hello'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    print('%')\n",
        "    q = 'How are you'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    print('%')\n",
        "    q= 'Are you my friend'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    print('%')\n",
        "    q = 'What are you doing'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    print('%')\n",
        "    q = 'What your favorite restaurant'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    print('%')\n",
        "    q = 'Who are you'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    print('%')\n",
        "    q = 'Do you want to go out'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    print('#'*20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nlu-Pzhkz-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create plot to show the loss progress while training\n",
        "def plot_history():\n",
        "    plt.figure(figsize=(4,3))\n",
        "    plt.plot(best_ep,smallest_loss,'ro')\n",
        "    plt.plot(history['loss'],'b-')\n",
        "    plt.legend(['best','loss'])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVMnvPdgvY2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reload a checkpoint if training was interrupted\n",
        "last_stopped = 0\n",
        "try:\n",
        "  checkpoint.restore(snapshot_folder+'/'+str(emb_dim)+\"-ckpt-\"+str(last_stopped))\n",
        "  print('Successfully loaded epoche' + str(last_stopped))\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "colab": {}
      },
      "source": [
        "batch_loss = K.constant(0)\n",
        "X, y = [], []\n",
        "\n",
        "#Iterate over the training epoches\n",
        "for ep in range(current_ep,EPOCHS-last_stopped):\n",
        "    current_ep = ep    \n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    btch = 1\n",
        "\n",
        "    #Iterates over each pair of conversation\n",
        "    for p in pairs_final_train:     \n",
        "        \n",
        "        #Split the conversation into message and response\n",
        "        question = p[0]\n",
        "        label = p[1]\n",
        "\n",
        "        #Create lists of indices out of the sentences\n",
        "        question_seq = [wordtoix[word] for word in question.split(' ') if word in wordtoix]\n",
        "        label_seq = [wordtoix[word] for word in label.split(' ') if word in wordtoix]\n",
        "\n",
        "        #Padding of the sentences to the max length\n",
        "        enc_in_seq = pad_sequences([question_seq], maxlen=max_len_q, padding='post')[0]\n",
        "        dec_out_seq = pad_sequences([label_seq], maxlen=max_len_a, padding='post')[0]\n",
        "        \n",
        "        X.append(enc_in_seq)\n",
        "        y.append(dec_out_seq)\n",
        "\n",
        "\n",
        "        if len(X) == batch_size:\n",
        "            #Put the batch sized input and output arrays and the enc_hidden into the training step\n",
        "            batch_loss = train_step(np.array(X), np.array(y), enc_hidden)\n",
        "\n",
        "            #Sum up to the total loss in each step\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            #Clear X and y to be able to create the next batch\n",
        "            X , y = [], []\n",
        "            btch += 1\n",
        "\n",
        "            #After calculating to given number of batches per epoche, print metadata and loss\n",
        "            if btch % (steps_per_epoch//batch_per_epoche) == 0:\n",
        "                print('Epoch {} Batch {} Loss: {:.4f}'.format(ep + last_stopped , btch, K.get_value(batch_loss)))\n",
        "\n",
        "    #Calculate average loss of each epoches step\n",
        "    epoch_loss =  K.get_value(total_loss) / steps_per_epoch\n",
        "    print('\\n*** Epoch {} Loss {:.4f} ***\\n'.format(ep + last_stopped,epoch_loss))\n",
        "    history['loss'].append(epoch_loss)\n",
        "    \n",
        "    #Save the model of the current epoche\n",
        "    checkpoint.save(file_prefix = '/content/drive/My Drive/Data Exploration Project')\n",
        "\n",
        "    #Show how the bot is performing right now\n",
        "    test_bot()\n",
        "\n",
        "    #Track each epoches average loss to find the best epoche while training\n",
        "    if epoch_loss < smallest_loss:\n",
        "        smallest_loss = epoch_loss\n",
        "        best_ep = ep \n",
        "        print('Reached a new best loss!')\n",
        "    \n",
        "    #plot after each third epoche\n",
        "    if ep % 3 == 0:\n",
        "        plot_history()\n",
        "        \n",
        "    print('Best epoch so far: ',best_ep,' smallest loss:',smallest_loss)\n",
        "    print('Time taken for the epoch {:.3f} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    print('=' * 40)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwaeAFH4kz-s",
        "colab_type": "text"
      },
      "source": [
        "It appears that themodel is not getting better anymore, it is overfitting. so I stop the training here and I use the check point at epoch 82 as the inference model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NxO_eIGkz-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXPkXAZ6kz-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint.restore(snapshot_folder+'/'+str(emb_dim)+\"-ckpt-2\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSJyCyUekz-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnzI0eZ0kz-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = \"I like you\"\n",
        "answer(q, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIiOe6lIkz-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = \"How do you answer\"\n",
        "answer(q, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Kcv2JZkz-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = \"are you hungry\"\n",
        "answer(q, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMaU6jZSkz-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = \"who are you\"\n",
        "answer(q, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpBdNjXzkz-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = \"Do you drink\"\n",
        "answer(q, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}